---
title: Upgraded DPDK support in oVirt
author: lgoldber
tags: OVS-DPDK, Open vSwitch, OVN, ansible
date: 2018-07-19 10:00:00 CET
comments: true
published: true
---
[DPDK](http://dpdk.org/) (Data Plane Development Kit) provides high-performance packet processing libraries and user space drivers.

[Support for DPDK](https://www.ovirt.org/blog/2017/09/ovs-dpdk/) in oVirt was introduced in 2017, and is now upgraded in terms of both deployment via Ansible and usage via [Open Virtual Network](http://www.ovn.org/).

While still experimental, OVN-DPDK in oVirt is now available in versions 4.2 and upwards.

# What's changed?

### Ansible DPDK host setup

Host configuration for DPDK usage is now automated using Ansible. This primarly includes:
* Hugepages configuration -- hugepage size and quantity in the kernel.
* CPU partitioning.
* Binding NICs to userspace drivers. 
* OVS-DPDK related configuration (initialization, socket memory, pmd thread core binding, etc).

The role is installed via Ansible galaxy:
```
# ansible-galaxy install oVirt.dpdk-setup
```
An example playbook:
```
- hosts: dpdk_host_0
  vars:
    pci_drivers:
      "0000:02:00.1": "vfio-pci"
      "0000:02:00.2": "igb"
      "0000:02:00.3": ""
    configure_kernel: true
    bind_drivers: true
    set_ovs_dpdk: false
  roles:
    - ovirt-ansible-dpdk-setup
```
The role is controlled by 3 boolean variables (all set to `true` by default) and a dictionary of devices and their drivers:
* `configure_kernel` -- determines whether the kernel should be configured for DPDK usage (hugepages, CPU partitioning). WARNING: When set to `true` it is very likely to trigger reboot of the host, unless all required configuration is already pre-set.
* `bind_drivers` -- determines whether the role should bind devices to their specified drivers.
* `set_ovs_dpdk` -- determines whether the role should initialize and configure OVS for DPDK usage.
* `pci_drivers` -- a dictionary of the PCI addresses of network interface cards as keys and drivers as values. Empty strings represent kernel's default. non-DPDK compatible drivers may be set; NICs not using DPDK compatible drivers won't be configured for OVS-DPDK usage and won't have their CPU isolated in multiple NUMA environments.

Additionally, there are several optional performance related variables:
* `pmd_threads_count` -- determines the number of PMD threads per NIC (default: `1`).
* `nr_2mb_hugepages` -- determines the number of 2MB hugepages, if 2MB hugepages are to be used (default: `1024`).
* `nr_1gb_hugepages` -- determines the number of 1GB hugepages, if 1GB hugepages are to be used (default: `4`).
* `use_1gb_hugepages` -- determines whether 1GB hugepages should be used, where 1GB hugepages are supported (default: `true`).

For additional information, refer to the role's [repository](https://github.com/ovirt/ovirt-ansible-dpdk-setup/).

### OVN Integration
DPDK in oVirt now leverages the capabilities of OVN in the form of an OVN localnet. This allows for seamless connectivity across OVN networks, benefiting from OVN's software defined routers, switches, security groups, and ACL's.

For more information about OVN localnet integration in oVirt, refer to oVirt's [Provider Physical Network RFE](https://ovirt.org/develop/release-management/features/network/provider-physical-network/).

# Usage in oVirt
1) Install `oVirt.dpdk-setup` Ansible role via ansible-galaxy:
    ```
    # ansible-galaxy install oVirt.dpdk-setup
    ```
2) Execute the role as described above.
3) Unless the host was already configured, the host will restart for kernel changes to be applied. Note that after reboot the following values are changed (If they are not, you may have incompatible hardware, or you should report a bug on ovirt@users.org):
    * IOMMU: ```/sys/devices/virtual/iommu/<DMAR device>/devices/<PCI address>``` should exist. DMAR devices are typically annotated by dmar0, dmar1, and so forth.
    * hugepages: ```grep Huge /proc/meminfo``` should reflect the desired state as defined in the Ansible playbook (i.e. `Hugepagesize` and `HugePages_Total`)
    * CPU partitioning: based on the devices that are to be used with a DPDK compatible driver, CPUs should be partitioned separately between each NUMA node. Refer to `lscpu` to see live CPU NUMA separation information, e.g.: `NUMA node1 CPU(s):     8-15`

    Note: NICs using userspace drivers do not appear in the kernel; `ip` commands won't list devices using userspace drivers. in oVirt, such devices appear as `dpdk0`, `dpdk1`, and so forth.
4) Create an oVirt network ("phys-net-demo") on your data center and attach it to your cluster.
![](https://www.ovirt.org/images/create_phys_net.png)
5) Attach "phys-net-demo" to DPDK NICs accross your cluster.
![](https://www.ovirt.org/images/set_phys_net.png)
6) Create an external OVN network ("ext-net-demo"), setting "phys-net-demo" as its physical network. This configuration between "ext-net-demo" and "phys-net-demo" will enable traffic between "phys-net-demo" (which DPDK's physical port is part of) and the rest of the ports present in OVN's "ext-net-demo" network.
![](https://www.ovirt.org/images/create_ext_net.png)
7) "ext-net-demo" vNic's may now be added to virtual machines. These vNics now share L2 connectivity with other remote ports in "ext-net-demo" via DPDK's NIC (and other local ports internally).
![](https://www.ovirt.org/images/ext_net_demo.png)
8) Host to VM packets are transmitted and received on buffers allocated on shared hugepages memory. Virtual machines using DPDK based NIC's need the following custom properties set:

    *`hugepages_shared` --  `true`.
    *`hugepages` -- the number of hugepages to share.
![](https://www.ovirt.org/images/hugepages.png)
    If either of the custom properties are missing, add them to engine using `engine-config` followed by engine restart:
    `engine-config -s "UserDefinedVMProperties=hugepages=^.*$;hugepages_shared=(true|false)"`
    `systemctl restart ovirt-engine`
