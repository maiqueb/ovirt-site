---
title: Upgraded DPDK support in oVirt
author: lgoldber
tags: OVS-DPDK, Open vSwitch, OVN, ansible
date: 2018-07-19 10:00:00 CET
comments: true
published: true
---
[DPDK](http://dpdk.org/) (Data Plane Development Kit) provides high-performance packet processing libraries and user space drivers.

[Support for DPDK](https://www.ovirt.org/blog/2017/09/ovs-dpdk/) in oVirt was introduced in 2017, and is now upgraded in terms of both deployment via Ansible and usage via [Open Virtual Network](http://www.ovn.org/).

While still experimental, OVN-DPDK in oVirt is now available in both the stable and master oVirt branches. 

# What's changed?

### Ansible DPDK host setup

Host configuration for DPDK usage is now automated using Ansible. This primarly includes:
* Hugepages configuration -- hugepage size and quantity in the kernel.
* CPU partitioning.
* NIC to userspace driver binding.
* OVS-DPDK related configuration (initialization, socket memory, pmd thread core binding, etc).

The role is installed via Ansible galaxy:
```
# ansible-galaxy install oVirt.dpdk-setup
```
An example playbook:
```
- hosts: dpdk_host_0
  vars:
    pci_drivers:
      "0000:02:00.1": "vfio-pci"
      "0000:02:00.2": "igb"
      "0000:02:00.3": ""
    configure_kernel: true
    bind_drivers: true
    set_ovs_dpdk: false
  roles:
    - ovirt-ansible-dpdk-setup
```
The role is controlled by 3 boolean variables (all set to `true` by default) and a dictionary of devices and their drivers:
* `configure_kernel` -- determines whether the kernel should be configured for DPDK usage (hugepages, CPU partitioning)
* `bind_drivers` -- determines whether the role should bind devices to their specified drivers.
* `set_ovs_dpdk` -- determines whether the role should initialize and configure OVS for DPDK usage.
* `pci_drivers` -- a dictionary of the PCI addresses of network interface cards as keys and drivers as values. Empty strings represent the kernel default. non-DPDK compatible drivers won't be configured for OVS-DPDK usage.

Additionally, there are several optional performance related variables:
* `pmd_threads_count` -- determines the amount of PMD threads per NIC (default: `1`).
* `nr_2mb_hugepages` -- determines the amount of 2MB hugepages, if 2MB hugepages are to be used (default: `1024`).
* `nr_1gb_hugepages` -- determines the amount of 1GB hugepages, if 1GB hugepages are to be used (default: `4`).
* `use_1gb_hugepages` -- determines whether 1GB hugepages should be used, where 1GB hugepages are supported (default: `true`).

For additional information, refer to the role's [repository](https://github.com/ovirt/ovirt-ansible-dpdk-setup/).

### OVN Integration
DPDK in oVirt now leverages the capabilities of OVN in the form of an OVN localnet. This allows for seamless connectivity across OVN networks, benefiting from OVN's software defined routers, switches, security groups, and ACL's.

For more information about OVN localnet integration in oVirt, refer to oVirt's [Provider Physical Network RFE](https://ovirt.org/develop/release-management/features/network/provider-physical-network/).

# Usage in oVirt
1) Install `oVirt.dpdk-setup` Ansible role via ansible-galaxy:
    ```
    # ansible-galaxy install oVirt.dpdk-setup
    ```
2) Execute the role as described above.
3) Unless the host was already configured, the host will restart for kernel changes to be applied.
4) Make sure the following:
    * IOMMU: ```/sys/devices/virtual/iommu/<DMAR device>/devices/<PCI address>``` should exist. DMAR devices are typically annotated by dmar0, dmar1, and so forth.
    * hugepages: ```grep Huge /proc/meminfo``` should reflect the desired state as defined in the Ansible playbook (i.e. `Hugepagesize` and `HugePages_Total`)
    * CPU partitioning: based on the devices that are to be used with a DPDK compatible driver, CPU cores should be partitioned separately on each NUMA node. Refer to `lscpu` to see live CPU NUMA separation information, e.g.: `NUMA node1 CPU(s):     8-15`
5) Create an oVirt network ("localnet-0"). This network will be used to attach a DPDK NIC.
6) Create an external OVN network ("external-net-0"), setting "localnet-0" as its physical network. This configuration between "external-net-0" and "localnet-0" will enable traffic between "localnet-0" (which DPDK's physical port is part of) and the rest of the ports present in OVN's "external-net-0" network.
7) "external-net-0" vNic's may now be added to virtual machines. These vNics now share L2 connectivity with other remote ports in "external-net-0" via DPDK's NIC (and other local ports internally).

Note:
* Virtual machines using DPDK based NIC's need to have both `hugepages_shared` (to `true`) and `hugepages` (to the amount of requested shared hugepages) custom properties set. Host to VM packets are transmitted and received on buffers allocated on shared hugepages memory. As of date, these properties are not automated.
